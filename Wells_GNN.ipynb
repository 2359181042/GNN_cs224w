{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wells_GNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPH75L9YLRzsPTYpCZifXlf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2359181042/GNN_cs224w/blob/main/Wells_GNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz4zi3-fdsdy",
        "outputId": "64bc137d-a7bd-49e5-d74d-88964a387ba5"
      },
      "source": [
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install pytorch_lightning"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6 MB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 222 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 376 kB 9.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.7 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\n",
            "\u001b[K     |████████████████████████████████| 813 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting tensorboard!=2.5.0,>=2.2.0\n",
            "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6 MB 44.7 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.2.0\n",
            "  Downloading torchmetrics-0.4.1-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 45.5 MB/s \n",
            "\u001b[?25hCollecting PyYAML<=5.4.1,>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 26.5 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.0\n",
            "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.41.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.0)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 66.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.9.0+cu102)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 45.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (2.4.7)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.34.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.32.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (57.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.12.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.3.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.36.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch_lightning) (3.7.4.3)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 71.5 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 40.7 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.5.0)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=9b20f556677ba468022f756f400f01ef2e79f889b27a52de4a342be0437e6888\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, torchmetrics, tensorboard, PyYAML, pyDeprecate, future, pytorch-lightning\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.5.0 requires tensorboard~=2.5, but you have tensorboard 2.4.1 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.7.0 future-0.18.2 multidict-5.1.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.8 tensorboard-2.4.1 torchmetrics-0.4.1 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7WrNvF2eKok",
        "outputId": "3c622b72-c0cb-4dc2-aef1-3fbc94edb2d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "DATA_PATH = 'gdrive/My Drive/DaguRiver/'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvtb1Ez-bYg8",
        "outputId": "3b936bd8-4361-4886-b05d-5825da04a07d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "import math\n",
        "import matplotlib\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "sns.set(style='whitegrid',palette='muted',font_scale = 1.2)\n",
        "HAPPY_COLORS_PALETTE = ['#01BEFE','#FFDD00','#FF7D00','#FF006D','#ADFF02','#8F00FF']\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "rcParams['figure.figsize']=12,8\n",
        "tqdm.pandas()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX_wCOWfdWbP"
      },
      "source": [
        "# 1. Data Processing\n",
        "## Totally 10 features: salinity, level, pump, infil, infil_s, rain,loc, duration, well_conn, well_weight.\n",
        "## sequence length is 18, features salinity-rain is based on 11 opration wells, while feaures loc is based on 1 observe well. The well_conn and well weight are calculated based on the relative distance between two wells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmSabta1g5pb"
      },
      "source": [
        "## 1.1 Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqnmEWR3dPpI"
      },
      "source": [
        "test1=pd.read_csv(os.path.join(DATA_PATH,'df_of_well.csv'))\n",
        "salinity=[]\n",
        "level=[]\n",
        "pump=[]\n",
        "infil=[]\n",
        "infil_s=[]\n",
        "rain=[]\n",
        "for i in range(21):\n",
        "    salinity.append(np.array(test1.iloc[:,0+i*6]))\n",
        "\n",
        "    level.append(np.array(test1.iloc[:,1+i*6]))\n",
        "\n",
        "    pump.append(np.array(test1.iloc[:,2+i*6]))\n",
        "\n",
        "    infil.append(np.array(test1.iloc[:,3+i*6]))\n",
        "\n",
        "    infil_s.append(np.array(test1.iloc[:,4+i*6]))\n",
        "\n",
        "    rain.append(np.array(test1.iloc[:,5+i*6]))\n",
        "salinity=np.array(salinity)\n",
        "level=np.array(level)\n",
        "pump = np.array(pump)\n",
        "infil = np.array(infil)\n",
        "infil_s = np.array(infil_s)\n",
        "rain=np.array(rain)            "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G7UqS3Mfxsc",
        "outputId": "47357966-d53b-449f-a135-1bc82696b152",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df3=pd.read_csv(os.path.join(DATA_PATH,'date_info.csv'))\n",
        "df3['Date'] = pd.to_datetime(df3['Date'])\n",
        "\n",
        "df3['Day'] = df3['Date'].dt.day\n",
        "df3['Month']=df3['Date'].dt.month\n",
        "\n",
        "\n",
        "df3['Duration'] = df3['Day']-17 + 30*(df3['Month']-9) \n",
        "duration = df3['Duration'].to_numpy()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1  1  1  1  1  1  1  1  2  2  2  2  2  3  3  3  3  3  4  4  4  4  4  4\n",
            "  5  5  5  5  6  6  6  7  7  7  7  8  8  8  9  9  9  9 10 10 10 11 11 11\n",
            " 12 12 12 13 13 13 13 14 14 14 14 15 15 15 15 16 16 16 17 17 17 18 18 18\n",
            " 19 19 19 20 20 20 21 21 21]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STvUW14Fe3SL"
      },
      "source": [
        "Well_location= pd.read_csv(os.path.join(DATA_PATH,'sets.csv'))\n",
        "\n",
        "NE=[]\n",
        "North=[]\n",
        "East=[]\n",
        "for i in range(1,22):\n",
        "    N=Well_location.iloc[0][i]\n",
        "    North.append(N)\n",
        "    E=Well_location.iloc[1][i]\n",
        "    East.append(E)\n",
        "    NE.append((N,E))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z_zrL9lgjWK"
      },
      "source": [
        "from math import sin, cos, sqrt, atan2, radians\n",
        "R = 6378.1370\n",
        "def get_conn_attr(N,E):\n",
        "    Distance = []\n",
        "    con_i=[]\n",
        "    con_j=[]\n",
        "    for i in range(len(N)):\n",
        "        for j in range(len(N)):\n",
        "                lat1 = radians(N[i])\n",
        "                lon1 = radians(E[i])\n",
        "                lat2 = radians(N[j])\n",
        "                lon2 = radians(E[j])\n",
        "                dlon = lon2 - lon1\n",
        "                dlat = lat2 - lat1\n",
        "                a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
        "                c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
        "                distance = R * c\n",
        "                if (distance < 0.1) & (distance > 0):\n",
        "                    con_i.append(i)\n",
        "                    con_j.append(j)\n",
        "                    Distance.append(distance)\n",
        "\n",
        "    conn=torch.tensor([con_i,con_j])\n",
        "    edge_attr = torch.tensor([Distance])\n",
        "    edge_attr = edge_attr.transpose(0, 1) \n",
        "    edge_attr = edge_attr.unsqueeze(0).repeat(18,1,1)\n",
        "    return conn, edge_attr"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oizKgeCg3rj"
      },
      "source": [
        "## 1.2 Create dataset\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqLKVI4mguVB"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import sin, cos, sqrt, atan2, radians\n",
        "\n",
        "TIME_WINDOW = 18\n",
        "PRED_TIME = 18\n",
        "##well o 2 4 5  6  8 10 for train_set, 3 9 for validation set, 7 11 for test set\n",
        "## North[]. East[] 21 用于传入get_conn_attr 计算不同的图数据\n",
        " \n",
        "\n",
        "def create_dataset(salinity,level,pump,infil,infil_s,rain,NE,duration):\n",
        "    index=0\n",
        "    train_sequences=[]\n",
        "    val_sequences=[]\n",
        "    test_sequences=[]\n",
        "#O2\n",
        "    while (index <= (81-PRED_TIME-TIME_WINDOW)):\n",
        "\n",
        "        s=salinity[11:21,index:index+TIME_WINDOW]\n",
        "        l=level[11:21,index:index+TIME_WINDOW]\n",
        "        p=pump[11:21,index:index+TIME_WINDOW]\n",
        "        i=infil[11:21,index:index+TIME_WINDOW]\n",
        "        i_s=infil_s[11:21,index:index+TIME_WINDOW]\n",
        "        r = rain[11:21,index:index+TIME_WINDOW]\n",
        "        N0= North[11:21]+[North[0]]\n",
        "        E0= East[11:21]+[East[0]]\n",
        "        conn,edge_attr = get_conn_attr(N0,E0)\n",
        "        \n",
        "        loc = NE[0]\n",
        "        t=duration[index]\n",
        "        y = salinity[0,index+TIME_WINDOW:index+TIME_WINDOW+PRED_TIME]\n",
        "        \n",
        "        \n",
        "        \n",
        "        train_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "    while ((81-PRED_TIME-TIME_WINDOW)<index<81):\n",
        "        index +=1\n",
        "\n",
        "        \n",
        "#O3        \n",
        "    while ( 81<=index<=(2*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[11:21,index-1*81:index+TIME_WINDOW-1*81]\n",
        "        l=level[11:21,index-1*81:index+TIME_WINDOW-1*81]\n",
        "        p=pump[11:21,index-1*81:index+TIME_WINDOW-1*81]\n",
        "        i=infil[11:21,index-1*81:index+TIME_WINDOW-1*81]\n",
        "        i_s=infil_s[11:21,index-1*81:index+TIME_WINDOW-1*81]\n",
        "        r = rain[11:21,index-1*81:index+TIME_WINDOW-1*81]\n",
        "        N1= North[11:21]+[North[1]]\n",
        "        E1= East[11:21]+[East[1]]\n",
        "        conn,edge_attr = get_conn_attr(N1,E1)\n",
        "        loc = NE[1]\n",
        "        t=duration[index-81]\n",
        "        y = salinity[1,index-1*81+TIME_WINDOW:index-1*81+TIME_WINDOW+PRED_TIME]\n",
        "        val_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "\n",
        "    while ((2*81-PRED_TIME-TIME_WINDOW)<index<2*81):\n",
        "        index +=1    \n",
        "\n",
        "#O4\n",
        "    while ( 2*81<=index<=(3*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[11:21,index-2*81:index+TIME_WINDOW-2*81]\n",
        "        l=level[11:21,index-2*81:index+TIME_WINDOW-2*81]\n",
        "        p=pump[11:21,index-2*81:index+TIME_WINDOW-2*81]\n",
        "        i=infil[11:21,index-2*81:index+TIME_WINDOW-2*81]\n",
        "        i_s=infil_s[11:21,index-2*81:index+TIME_WINDOW-2*81]\n",
        "        r = rain[11:21,index-2*81:index+TIME_WINDOW-2*81]\n",
        "        N2= North[11:21]+[North[2]]\n",
        "        E2= East[11:21]+[East[2]]\n",
        "        conn,edge_attr = get_conn_attr(N2,E2)\n",
        "        loc = NE[2]\n",
        "        t=duration[index-81*2]\n",
        "        y = salinity[2,index-2*81+TIME_WINDOW:index-2*81+TIME_WINDOW+PRED_TIME]\n",
        "        train_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "\n",
        "    while ((3*81-PRED_TIME-TIME_WINDOW)<index<3*81):\n",
        "        index +=1     \n",
        "\n",
        "#O5 print('O4finish')      \n",
        "    while ( 3*81<=index<=(4*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[11:21,index-3*81:index-3*81+TIME_WINDOW]\n",
        "        l=level[11:21,index-3*81:index-3*81+TIME_WINDOW]\n",
        "        p=pump[11:21,index-3*81:index-3*81+TIME_WINDOW]\n",
        "        i=infil[11:21,index-3*81:index-3*81+TIME_WINDOW]\n",
        "        i_s=infil_s[11:21,index-3*81:index-3*81+TIME_WINDOW]\n",
        "        r = rain[11:21,index-3*81:index-3*81+TIME_WINDOW]\n",
        "        N3= North[11:21]+[North[3]]\n",
        "        E3= East[11:21]+[East[3]]\n",
        "        conn,edge_attr = get_conn_attr(N3,E3)\n",
        "        loc = NE[3]\n",
        "        t=duration[index-81*3]\n",
        "        y = salinity[3,index-3*81+TIME_WINDOW:index-3*81+TIME_WINDOW+PRED_TIME]\n",
        "        train_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "\n",
        "    while ((4*81-PRED_TIME-TIME_WINDOW)<index<4*81):\n",
        "        index +=1           \n",
        "#O6       \n",
        "    while ( 4*81<=index<=(5*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[11:21,index-4*81:index-4*81+TIME_WINDOW]\n",
        "        l=level[11:21,index-4*81:index-4*81+TIME_WINDOW]\n",
        "        p=pump[11:21,index-4*81:index-4*81+TIME_WINDOW]\n",
        "        i=infil[11:21,index-4*81:index-4*81+TIME_WINDOW]\n",
        "        i_s=infil_s[11:21,index-4*81:index-4*81+TIME_WINDOW]\n",
        "        r = rain[11:21,index-4*81:index-4*81+TIME_WINDOW]\n",
        "        N4= North[11:21]+[North[4]]\n",
        "        E4= East[11:21]+[East[4]]\n",
        "        conn,edge_attr = get_conn_attr(N4,E4)\n",
        "        loc = NE[4]\n",
        "        t=duration[index-81*4]\n",
        "        y = salinity[4,index-4*81+TIME_WINDOW:index-4*81+TIME_WINDOW+PRED_TIME]\n",
        "        train_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "\n",
        "    while ((5*81-PRED_TIME-TIME_WINDOW)<index<5*81):\n",
        "        index +=1  \n",
        "\n",
        "#O7       \n",
        "    while ( 5*81<=index<=(6*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[11:21,index-5*81:index-5*81+TIME_WINDOW]\n",
        "        l=level[11:21,index-5*81:index-5*81+TIME_WINDOW]\n",
        "        p=pump[11:21,index-5*81:index-5*81+TIME_WINDOW]\n",
        "        i=infil[11:21,index-5*81:index-5*81+TIME_WINDOW]\n",
        "        i_s=infil_s[11:21,index-5*81:index-5*81+TIME_WINDOW]\n",
        "        r = rain[11:21,index-5*81:index-5*81+TIME_WINDOW]\n",
        "        N5= North[11:21]+[North[5]]\n",
        "        E5= East[11:21]+[East[5]]\n",
        "        conn,edge_attr = get_conn_attr(N5,E5)\n",
        "        loc = NE[5]\n",
        "        t=duration[index-81*5]\n",
        "        y = salinity[5,index-5*81+TIME_WINDOW:index-5*81+TIME_WINDOW+PRED_TIME]\n",
        "        test_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "\n",
        "    while ((6*81-PRED_TIME-TIME_WINDOW)<index<6*81):\n",
        "        index +=1 \n",
        "#O8      \n",
        "    while ( 6*81<=index<=(7*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[11:21,index-6*81:index-6*81+TIME_WINDOW]\n",
        "        l=level[11:21,index-6*81:index-6*81+TIME_WINDOW]\n",
        "        p=pump[11:21,index-6*81:index-6*81+TIME_WINDOW]\n",
        "        i=infil[11:21,index-6*81:index-6*81+TIME_WINDOW]\n",
        "        i_s=infil_s[11:21,index-6*81:index-6*81+TIME_WINDOW]\n",
        "        r = rain[11:21,index-6*81:index-6*81+TIME_WINDOW]\n",
        "        N6= North[11:21]+[North[6]]\n",
        "        E6= East[11:21]+[East[6]]\n",
        "        conn,edge_attr = get_conn_attr(N6,E6)\n",
        "        loc = NE[6]\n",
        "        t=duration[index-81*6]\n",
        "        y = salinity[6,index-6*81+TIME_WINDOW:index-6*81+TIME_WINDOW+PRED_TIME]\n",
        "        train_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "\n",
        "    while ((7*81-PRED_TIME-TIME_WINDOW)<index<7*81):\n",
        "        index +=1         \n",
        "#O9        \n",
        "    while ( 7*81<=index<=(8*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[11:21,index-7*81:index-7*81+TIME_WINDOW]\n",
        "        l=level[11:21,index-7*81:index-7*81+TIME_WINDOW]\n",
        "        p=pump[11:21,index-7*81:index-7*81+TIME_WINDOW]\n",
        "        i=infil[11:21,index-7*81:index-7*81+TIME_WINDOW]\n",
        "        i_s=infil_s[11:21,index-7*81:index-7*81+TIME_WINDOW]\n",
        "        r = rain[11:21,index-7*81:index-7*81+TIME_WINDOW]\n",
        "        N7= North[11:21]+[North[7]]\n",
        "        E7= East[11:21]+[East[7]]\n",
        "        conn,edge_attr = get_conn_attr(N7,E7)\n",
        "        loc = NE[7]\n",
        "        t=duration[index-81*7]\n",
        "        y = salinity[7,index-7*81+TIME_WINDOW:index-7*81+TIME_WINDOW+PRED_TIME]\n",
        "        val_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "\n",
        "    while ((8*81-PRED_TIME-TIME_WINDOW)<index<8*81):\n",
        "        index +=1 \n",
        "#O10        \n",
        "    while ( 8*81<=index<=(9*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[11:21,index-8*81:index-8*81+TIME_WINDOW]\n",
        "        l=level[11:21,index-8*81:index-8*81+TIME_WINDOW]\n",
        "        p=pump[11:21,index-8*81:index-8*81+TIME_WINDOW]\n",
        "        i=infil[11:21,index-8*81:index-8*81+TIME_WINDOW]\n",
        "        i_s=infil_s[11:21,index-8*81:index-8*81+TIME_WINDOW]\n",
        "        r = rain[11:21,index-8*81:index-8*81+TIME_WINDOW]\n",
        "        N8= North[11:21]+[North[8]]\n",
        "        E8= East[11:21]+[East[8]]\n",
        "        conn,edge_attr = get_conn_attr(N8,E8)\n",
        "        loc = NE[8]\n",
        "        t=duration[index-81*8]\n",
        "        y = salinity[8,index-8*81+TIME_WINDOW:index-8*81+TIME_WINDOW+PRED_TIME]\n",
        "        train_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "    while ((9*81-PRED_TIME-TIME_WINDOW)<index<9*81):\n",
        "        index +=1 \n",
        "        \n",
        "#O11        \n",
        "    while ( 9*81<=index<=(10*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[11:21,index-9*81:index-9*81+TIME_WINDOW]\n",
        "        l=level[11:21,index-9*81:index-9*81+TIME_WINDOW]\n",
        "        p=pump[11:21,index-9*81:index-9*81+TIME_WINDOW]\n",
        "        i=infil[11:21,index-9*81:index-9*81+TIME_WINDOW]\n",
        "        i_s=infil_s[11:21,index-9*81:index-9*81+TIME_WINDOW]\n",
        "        r = rain[11:21,index-9*81:index-9*81+TIME_WINDOW]\n",
        "        N9= North[11:21]+[North[9]]\n",
        "        E9= East[11:21]+[East[9]]\n",
        "        conn,edge_attr = get_conn_attr(N9,E9)\n",
        "        loc = NE[9]\n",
        "        t=duration[index-81*9]\n",
        "        y = salinity[9,index-9*81+TIME_WINDOW:index-9*81+TIME_WINDOW+PRED_TIME]\n",
        "        test_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "    while ((10*81-PRED_TIME-TIME_WINDOW)<index<10*81):\n",
        "        index +=1 \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "    return train_sequences,val_sequences,test_sequences"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIKMVNachP_j",
        "outputId": "9ac939b6-ff2e-48c9-b7d2-33ccf6384edd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_sequences,val_sequences, test_sequences = create_dataset(salinity,level,pump,infil,infil_s,rain,NE,duration)\n",
        "print(len(train_sequences))\n",
        "print(len(test_sequences))\n",
        "print(len(val_sequences))\n",
        "print(train_sequences[92][9].shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "276\n",
            "92\n",
            "92\n",
            "torch.Size([18, 22, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQrltlNhhw3p"
      },
      "source": [
        "## 1.3 Create datamodule and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_QpOuuKhl6z"
      },
      "source": [
        "class WellDataset(Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        self.sequences = sequences\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    def __getitem__(self,index):\n",
        "        sal,lev,pum,inf,inf_s,rai,loca,dur,conn,edge_attr, pred_y = self.sequences[index]\n",
        "        return dict(\n",
        "            sal_sequence = torch.tensor(sal),\n",
        "            lev_sequence = torch.tensor(lev),\n",
        "            pum_sequence = torch.tensor(pum),\n",
        "            inf_sequence = torch.tensor(inf),\n",
        "            inf_s_sequence = torch.tensor(inf_s),\n",
        "            rai_sequence = torch.tensor(rai),\n",
        "            loc_sequence = torch.tensor(loca),\n",
        "            dur_sequence = torch.tensor(dur),\n",
        "            con_sequence = conn,\n",
        "            edg_sequence = edge_attr,\n",
        "            \n",
        "            label = torch.tensor(pred_y).float()   \n",
        "        )\n",
        "\n",
        "    \n",
        "\n",
        "class WellDataModule(pl.LightningDataModule):\n",
        "    \n",
        "    def __init__(\n",
        "        self, train_sequences, val_sequences, test_sequences, batch_size=45\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.train_sequences = train_sequences\n",
        "        self.val_sequences = val_sequences\n",
        "        self.test_sequences = test_sequences\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "    def setup(self):\n",
        "        self.train_dataset = WellDataset(self.train_sequences)\n",
        "        self.val_dataset = WellDataset(self.val_sequences)\n",
        "        self.test_dataset = WellDataset(self.test_sequences)\n",
        "    def train_dataloader(self):\n",
        "        return Dataloader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle = False\n",
        "        )\n",
        "    def val_dataloader(self):\n",
        "        return Dataloader(\n",
        "        self.val_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle = False\n",
        "        )\n",
        "    def test_dataloader(self):\n",
        "        return Dataloader(\n",
        "        self.test_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle = False\n",
        "        )"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUsG9Fgwh8v1"
      },
      "source": [
        "N_EPOCHS=8\n",
        "BATCH_SIZE= 46\n",
        "##We set the batchsize within a single ob well data, which is 81-18-18+1=46. \n",
        "data_module = WellDataModule(train_sequences,val_sequences, test_sequences,batch_size =BATCH_SIZE )\n",
        "data_module.setup()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPoQ7mQDiFS3",
        "outputId": "6c210909-85d3-4840-a39b-1fa8c0fe9294",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dataset =  WellDataset(train_sequences)\n",
        "for item in train_dataset:\n",
        "    print(item.keys())\n",
        "    print(item['sal_sequence'].shape)\n",
        "    break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['sal_sequence', 'lev_sequence', 'pum_sequence', 'inf_sequence', 'inf_s_sequence', 'rai_sequence', 'loc_sequence', 'dur_sequence', 'con_sequence', 'edg_sequence', 'label'])\n",
            "torch.Size([10, 18])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QuugqK4iHtq"
      },
      "source": [
        "# 2.Model"
      ]
    }
  ]
}