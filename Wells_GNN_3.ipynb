{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wells_GNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyME+vZo040pyj8VLJjTi6gG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2359181042/GNN_cs224w/blob/main/Wells_GNN_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz4zi3-fdsdy",
        "outputId": "5e71974a-44fe-4995-d3af-bbda33f236f8"
      },
      "source": [
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install pytorch_lightning"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 6.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 222 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 376 kB 9.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.4 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\n",
            "\u001b[K     |████████████████████████████████| 813 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (7.1.2)\n",
            "Collecting PyYAML<=5.4.1,>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 18.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.9.0+cu102)\n",
            "Collecting tensorboard!=2.5.0,>=2.2.0\n",
            "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6 MB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.41.1)\n",
            "Collecting torchmetrics>=0.2.0\n",
            "  Downloading torchmetrics-0.4.1-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 46.0 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.0\n",
            "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 48.7 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 56.2 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (2.4.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.32.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (57.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.34.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.8.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch_lightning) (3.7.4.3)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 73.0 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 46.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.5.0)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=3f93a8d483b171e1668e64a087c96004368848a6fd38f4fd4a8def464d6c1b1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, torchmetrics, tensorboard, PyYAML, pyDeprecate, future, pytorch-lightning\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.5.0 requires tensorboard~=2.5, but you have tensorboard 2.4.1 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.7.0 future-0.18.2 multidict-5.1.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.8 tensorboard-2.4.1 torchmetrics-0.4.1 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7WrNvF2eKok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8511c7d-8c27-47c6-f5f2-23970e0564e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "DATA_PATH = 'gdrive/My Drive/DaguRiver/'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "nvtb1Ez-bYg8",
        "outputId": "8ad60ca5-f520-4f33-f47e-4af62222d683"
      },
      "source": [
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "import math\n",
        "import matplotlib\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "sns.set(style='whitegrid',palette='muted',font_scale = 1.2)\n",
        "HAPPY_COLORS_PALETTE = ['#01BEFE','#FFDD00','#FF7D00','#FF006D','#ADFF02','#8F00FF']\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "rcParams['figure.figsize']=12,8\n",
        "tqdm.pandas()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c218bbfde24d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloggers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorBoardLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX_wCOWfdWbP"
      },
      "source": [
        "# 1. Data Processing\n",
        "## Totally 10 features: salinity, level, pump, infil, infil_s, rain,loc, duration, well_conn, well_weight.\n",
        "## sequence length is 18, features salinity-rain is based on 11 opration wells, while feaures loc is based on 1 observe well. The well_conn and well weight are calculated based on the relative distance between two wells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmSabta1g5pb"
      },
      "source": [
        "## 1.1 Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqnmEWR3dPpI",
        "outputId": "09c2eff5-6d2b-4188-a084-d481bc9c3599"
      },
      "source": [
        "test1=pd.read_csv(os.path.join(DATA_PATH,'df_of_well.csv'))\n",
        "salinity=[]\n",
        "level=[]\n",
        "pump=[]\n",
        "infil=[]\n",
        "infil_s=[]\n",
        "rain=[]\n",
        "for i in range(21):\n",
        "    salinity.append(np.array(test1.iloc[:,0+i*6]))\n",
        "\n",
        "    level.append(np.array(test1.iloc[:,1+i*6]))\n",
        "\n",
        "    pump.append(np.absolute(np.array(test1.iloc[:,2+i*6])))\n",
        "\n",
        "    infil.append(np.array(test1.iloc[:,3+i*6]))\n",
        "\n",
        "    infil_s.append(np.array(test1.iloc[:,4+i*6]))\n",
        "\n",
        "    rain.append(np.array(test1.iloc[:,5+i*6]))\n",
        "salinity=np.array(salinity)\n",
        "level=np.array(level)\n",
        "pump = np.array(pump)\n",
        "infil = np.array(infil)\n",
        "infil_s = np.array(infil_s)\n",
        "rain=np.array(rain)            \n",
        "print(pump[11])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.   38.3  38.3  38.3  38.3  38.3  38.3  38.3  37.   37.   37.   37.\n",
            " 37.   38.   38.   38.   38.   38.   35.86 35.86 35.86 35.86 35.86 35.86\n",
            " 39.86 39.86 39.86 39.86 38.12 38.12 38.12 35.66 35.66 35.66 35.66 35.01\n",
            " 35.01 35.01 36.96 36.96 36.96 36.96 35.66 35.66 35.66 35.66 35.66 35.66\n",
            "  0.    0.   37.   38.3  38.3  38.3  38.3  35.01 35.01 35.01 35.01 35.66\n",
            " 35.66 35.66 35.66 35.01 35.01 35.01 33.13 33.13 33.13 32.9  32.9  32.9\n",
            " 33.13 33.13 33.13 33.13 33.13 33.13 31.3  31.3  31.3 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G7UqS3Mfxsc"
      },
      "source": [
        "df3=pd.read_csv(os.path.join(DATA_PATH,'date_info.csv'))\n",
        "df3['Date'] = pd.to_datetime(df3['Date'])\n",
        "\n",
        "df3['Day'] = df3['Date'].dt.day\n",
        "df3['Month']=df3['Date'].dt.month\n",
        "\n",
        "\n",
        "df3['Duration'] = df3['Day']-17 + 30*(df3['Month']-9) \n",
        "duration = df3['Duration'].to_numpy()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STvUW14Fe3SL"
      },
      "source": [
        "Well_location= pd.read_csv(os.path.join(DATA_PATH,'sets.csv'))\n",
        "\n",
        "NE=[]\n",
        "North=[]\n",
        "East=[]\n",
        "for i in range(1,22):\n",
        "    N=Well_location.iloc[0][i]\n",
        "    North.append(N)\n",
        "    E=Well_location.iloc[1][i]\n",
        "    East.append(E)\n",
        "    NE.append((N,E))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z_zrL9lgjWK"
      },
      "source": [
        "from math import sin, cos, sqrt, atan2, radians\n",
        "R = 6378.1370\n",
        "def get_conn_attr(N,E):\n",
        "    Distance = []\n",
        "    con_i=[]\n",
        "    con_j=[]\n",
        "    for i in range(len(N)):\n",
        "        for j in range(len(N)):\n",
        "                lat1 = radians(N[i])\n",
        "                lon1 = radians(E[i])\n",
        "                lat2 = radians(N[j])\n",
        "                lon2 = radians(E[j])\n",
        "                dlon = lon2 - lon1\n",
        "                dlat = lat2 - lat1\n",
        "                a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
        "                c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
        "                distance = R * c\n",
        "                if (distance < 0.1) & (distance > 0):\n",
        "                    con_i.append(i)\n",
        "                    con_j.append(j)\n",
        "                    Distance.append(distance)\n",
        "\n",
        "    conn=torch.tensor([con_i,con_j])\n",
        "    edge_attr = torch.tensor([Distance])\n",
        "    edge_attr = edge_attr.transpose(0, 1) \n",
        "    edge_attr = edge_attr.unsqueeze(0).repeat(18,1,1)\n",
        "    return conn, edge_attr"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUluYvDkYbMF",
        "outputId": "bf5ea3ac-b920-41c0-b276-4c2a62274fa5"
      },
      "source": [
        "list1=NE[10:21]\n",
        "list2=[NE[0]]\n",
        "NE1=list1+list2\n",
        "print(len(NE1))\n",
        "list3=[NE[1]]\n",
        "print(list1+list3)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12\n",
            "[(36.361608000000004, 120.17305900000001), (36.360627, 120.17312), (36.359646000000005, 120.17381699999999), (36.357737, 120.173676), (36.35668, 120.170804), (36.359432, 120.176148), (36.358961, 120.176022), (36.358132, 120.175868), (36.356833, 120.17463400000001), (36.356055, 120.173933), (36.361231, 120.17415), (36.3612222, 120.170423)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfsOqoXKVTov",
        "outputId": "4e9bf441-4360-4c5d-bc94-3ad53e36829b"
      },
      "source": [
        "salinity[[10,11,12,13,14,15,16,17,18,19,20,1],80:80+TIME_WINDOW]"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.83 ],\n",
              "       [0.9  ],\n",
              "       [0.92 ],\n",
              "       [  nan],\n",
              "       [0.76 ],\n",
              "       [0.92 ],\n",
              "       [0.94 ],\n",
              "       [0.91 ],\n",
              "       [0.84 ],\n",
              "       [0.82 ],\n",
              "       [0.87 ],\n",
              "       [0.865]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oizKgeCg3rj"
      },
      "source": [
        "## 1.2 Create dataset\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqLKVI4mguVB"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import sin, cos, sqrt, atan2, radians\n",
        "\n",
        "TIME_WINDOW = 18\n",
        "PRED_TIME = 18\n",
        "##well o 2 4 5  6  8 10 for train_set, 3 9 for validation set, 7 11 for test set\n",
        "## North[]. East[] 21 用于传入get_conn_attr 计算不同的图数据\n",
        " \n",
        "\n",
        "def create_dataset(salinity,level,pump,infil,infil_s,rain,NE,duration):\n",
        "    index=0\n",
        "    train_sequences=[]\n",
        "    val_sequences=[]\n",
        "    test_sequences=[]\n",
        "#O2\n",
        "    while (index <= (81-PRED_TIME-TIME_WINDOW)):\n",
        "\n",
        "        s=salinity[[10,11,12,13,14,15,16,17,18,19,20,0],index:index+TIME_WINDOW]\n",
        "        print(len(s))\n",
        "        l=level[[10,11,12,13,14,15,16,17,18,19,20,0],index:index+TIME_WINDOW]\n",
        "        p=pump[[10,11,12,13,14,15,16,17,18,19,20,0],index:index+TIME_WINDOW]\n",
        "        i=infil[[10,11,12,13,14,15,16,17,18,19,20,0],index:index+TIME_WINDOW]\n",
        "        i_s=infil_s[[10,11,12,13,14,15,16,17,18,19,20,0],index:index+TIME_WINDOW]\n",
        "        r = rain[[10,11,12,13,14,15,16,17,18,19,20,0],index:index+TIME_WINDOW]\n",
        "        N0= North[10:21]+[North[0]]\n",
        "        E0= East[10:21]+[East[0]]\n",
        "        conn,edge_attr = get_conn_attr(N0,E0)\n",
        "        list1=NE[10:21]\n",
        "        list2=[NE[0]]\n",
        "        loc=list1+list2\n",
        "        print(len(loc))\n",
        "        t=duration[index]\n",
        "        y = salinity[0,index+TIME_WINDOW:index+TIME_WINDOW+PRED_TIME]\n",
        "        \n",
        "        \n",
        "        \n",
        "        train_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "    while ((81-PRED_TIME-TIME_WINDOW)<index<81):\n",
        "        index +=1\n",
        "\n",
        "        \n",
        "#O3        \n",
        "    while ( 81<=index<=(2*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[[10,11,12,13,14,15,16,17,18,19,20,1],index:index-1*81+TIME_WINDOW]\n",
        "        l=level[[10,11,12,13,14,15,16,17,18,19,20,1],index:index-1*81+TIME_WINDOW]\n",
        "        p=pump[[10,11,12,13,14,15,16,17,18,19,20,1],index:index-1*81+TIME_WINDOW]\n",
        "        i=infil[[10,11,12,13,14,15,16,17,18,19,20,1],index:index-1*81+TIME_WINDOW]\n",
        "        i_s=infil_s[[10,11,12,13,14,15,16,17,18,19,20,1],index:index-1*81+TIME_WINDOW]\n",
        "        r = rain[[10,11,12,13,14,15,16,17,18,19,20,1],index:index-1*81+TIME_WINDOW]\n",
        "        N1= North[10:21]+[North[1]]\n",
        "        E1= East[10:21]+[East[1]]\n",
        "        conn,edge_attr = get_conn_attr(N1,E1)\n",
        "        list1=NE[10:21]\n",
        "        list2=[NE[1]]\n",
        "        loc=list1+list2\n",
        "        t=duration[index-81]\n",
        "        y = salinity[1,index-1*81+TIME_WINDOW:index-1*81+TIME_WINDOW+PRED_TIME]\n",
        "        val_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "\n",
        "    while ((2*81-PRED_TIME-TIME_WINDOW)<index<2*81):\n",
        "        index +=1    \n",
        "\n",
        "#O4\n",
        "    while ( 2*81<=index<=(3*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[[10,11,12,13,14,15,16,17,18,19,20,2],index-2*81:index-2*81+TIME_WINDOW]\n",
        "        l=level[[10,11,12,13,14,15,16,17,18,19,20,2],index-2*81:index-2*81+TIME_WINDOW]\n",
        "        p=pump[[10,11,12,13,14,15,16,17,18,19,20,2],index-2*81:index-2*81+TIME_WINDOW]\n",
        "        i=infil[[10,11,12,13,14,15,16,17,18,19,20,2],index-2*81:index-2*81+TIME_WINDOW]\n",
        "        i_s=infil_s[[10,11,12,13,14,15,16,17,18,19,20,2],index-2*81:index-2*81+TIME_WINDOW]\n",
        "        r = rain[[10,11,12,13,14,15,16,17,18,19,20,2],index-2*81:index-2*81+TIME_WINDOW]\n",
        "        N2= North[10:21]+[North[2]]\n",
        "        E2= East[10:21]+[East[2]]\n",
        "        conn,edge_attr = get_conn_attr(N2,E2)\n",
        "        list1=NE[10:21]\n",
        "        list2=[NE[2]]\n",
        "        loc=list1+list2\n",
        "        t=duration[index-81*2]\n",
        "        y = salinity[2,index-2*81+TIME_WINDOW:index-2*81+TIME_WINDOW+PRED_TIME]\n",
        "        train_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "\n",
        "    while ((3*81-PRED_TIME-TIME_WINDOW)<index<3*81):\n",
        "        index +=1     \n",
        "\n",
        "#O5 print('O4finish')      \n",
        "    while ( 3*81<=index<=(4*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[[10,11,12,13,14,15,16,17,18,19,20,3],index-3*81:index-3*81+TIME_WINDOW]\n",
        "        l=level[[10,11,12,13,14,15,16,17,18,19,20,3],index-3*81:index-3*81+TIME_WINDOW]\n",
        "        p=pump[[10,11,12,13,14,15,16,17,18,19,20,3],index-3*81:index-3*81+TIME_WINDOW]\n",
        "        i=infil[[10,11,12,13,14,15,16,17,18,19,20,3],index-3*81:index-3*81+TIME_WINDOW]\n",
        "        i_s=infil_s[[10,11,12,13,14,15,16,17,18,19,20,3],index-3*81:index-3*81+TIME_WINDOW]\n",
        "        r = rain[[10,11,12,13,14,15,16,17,18,19,20,3],index-3*81:index-3*81+TIME_WINDOW]\n",
        "        N3= North[10:21]+[North[3]]\n",
        "        E3= East[10:21]+[East[3]]\n",
        "        conn,edge_attr = get_conn_attr(N3,E3)\n",
        "        list1=NE[10:21]\n",
        "        list2=[NE[3]]\n",
        "        loc=list1+list2\n",
        "        t=duration[index-81*3]\n",
        "        y = salinity[3,index-3*81+TIME_WINDOW:index-3*81+TIME_WINDOW+PRED_TIME]\n",
        "        train_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "\n",
        "    while ((4*81-PRED_TIME-TIME_WINDOW)<index<4*81):\n",
        "        index +=1           \n",
        "#O6       \n",
        "    while ( 4*81<=index<=(5*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[[10,11,12,13,14,15,16,17,18,19,20,4],index-4*81:index-4*81+TIME_WINDOW]\n",
        "        l=level[[10,11,12,13,14,15,16,17,18,19,20,4],index-4*81:index-4*81+TIME_WINDOW]\n",
        "        p=pump[[10,11,12,13,14,15,16,17,18,19,20,4],index-4*81:index-4*81+TIME_WINDOW]\n",
        "        i=infil[[10,11,12,13,14,15,16,17,18,19,20,4],index-4*81:index-4*81+TIME_WINDOW]\n",
        "        i_s=infil_s[[10,11,12,13,14,15,16,17,18,19,20,4],index-4*81:index-4*81+TIME_WINDOW]\n",
        "        r = rain[[10,11,12,13,14,15,16,17,18,19,20,4],index-4*81:index-4*81+TIME_WINDOW]\n",
        "        N4= North[10:21]+[North[4]]\n",
        "        E4= East[10:21]+[East[4]]\n",
        "        conn,edge_attr = get_conn_attr(N4,E4)\n",
        "        list1=NE[10:21]\n",
        "        list2=[NE[4]]\n",
        "        loc=list1+list2\n",
        "        t=duration[index-81*4]\n",
        "        y = salinity[4,index-4*81+TIME_WINDOW:index-4*81+TIME_WINDOW+PRED_TIME]\n",
        "        train_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "\n",
        "    while ((5*81-PRED_TIME-TIME_WINDOW)<index<5*81):\n",
        "        index +=1  \n",
        "\n",
        "#O7       \n",
        "    while ( 5*81<=index<=(6*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[[10,11,12,13,14,15,16,17,18,19,20,5],index-5*81:index-5*81+TIME_WINDOW]\n",
        "        l=level[[10,11,12,13,14,15,16,17,18,19,20,5],index-5*81:index-5*81+TIME_WINDOW]\n",
        "        p=pump[[10,11,12,13,14,15,16,17,18,19,20,5],index-5*81:index-5*81+TIME_WINDOW]\n",
        "        i=infil[[10,11,12,13,14,15,16,17,18,19,20,5],index-5*81:index-5*81+TIME_WINDOW]\n",
        "        i_s=infil_s[[10,11,12,13,14,15,16,17,18,19,20,5],index-5*81:index-5*81+TIME_WINDOW]\n",
        "        r = rain[[10,11,12,13,14,15,16,17,18,19,20,5],index-5*81:index-5*81+TIME_WINDOW]\n",
        "        N5= North[10:21]+[North[5]]\n",
        "        E5= East[10:21]+[East[5]]\n",
        "        conn,edge_attr = get_conn_attr(N5,E5)\n",
        "        list1=NE[10:21]\n",
        "        list2=[NE[5]]\n",
        "        loc=list1+list2\n",
        "        t=duration[index-81*5]\n",
        "        y = salinity[5,index-5*81+TIME_WINDOW:index-5*81+TIME_WINDOW+PRED_TIME]\n",
        "        test_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "\n",
        "    while ((6*81-PRED_TIME-TIME_WINDOW)<index<6*81):\n",
        "        index +=1 \n",
        "#O8      \n",
        "    while ( 6*81<=index<=(7*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[[10,11,12,13,14,15,16,17,18,19,20,6],index-6*81:index-6*81+TIME_WINDOW]\n",
        "        l=level[[10,11,12,13,14,15,16,17,18,19,20,6],index-6*81:index-6*81+TIME_WINDOW]\n",
        "        p=pump[[10,11,12,13,14,15,16,17,18,19,20,6],index-6*81:index-6*81+TIME_WINDOW]\n",
        "        i=infil[[10,11,12,13,14,15,16,17,18,19,20,6],index-6*81:index-6*81+TIME_WINDOW]\n",
        "        i_s=infil_s[[10,11,12,13,14,15,16,17,18,19,20,6],index-6*81:index-6*81+TIME_WINDOW]\n",
        "        r = rain[[10,11,12,13,14,15,16,17,18,19,20,6],index-6*81:index-6*81+TIME_WINDOW]\n",
        "        N6= North[10:21]+[North[6]]\n",
        "        E6= East[10:21]+[East[6]]\n",
        "        conn,edge_attr = get_conn_attr(N6,E6)\n",
        "        list1=NE[10:21]\n",
        "        list2=[NE[6]]\n",
        "        loc=list1+list2\n",
        "        t=duration[index-81*6]\n",
        "        y = salinity[6,index-6*81+TIME_WINDOW:index-6*81+TIME_WINDOW+PRED_TIME]\n",
        "        train_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "\n",
        "    while ((7*81-PRED_TIME-TIME_WINDOW)<index<7*81):\n",
        "        index +=1         \n",
        "#O9        \n",
        "    while ( 7*81<=index<=(8*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[[10,11,12,13,14,15,16,17,18,19,20,7],index-7*81:index-7*81+TIME_WINDOW]\n",
        "        l=level[[10,11,12,13,14,15,16,17,18,19,20,7],index-7*81:index-7*81+TIME_WINDOW]\n",
        "        p=pump[[10,11,12,13,14,15,16,17,18,19,20,7],index-7*81:index-7*81+TIME_WINDOW]\n",
        "        i=infil[[10,11,12,13,14,15,16,17,18,19,20,7],index-7*81:index-7*81+TIME_WINDOW]\n",
        "        i_s=infil_s[[10,11,12,13,14,15,16,17,18,19,20,7],index-7*81:index-7*81+TIME_WINDOW]\n",
        "        r = rain[[10,11,12,13,14,15,16,17,18,19,20,7],index-7*81:index-7*81+TIME_WINDOW]\n",
        "        N7= North[10:21]+[North[7]]\n",
        "        E7= East[10:21]+[East[7]]\n",
        "        conn,edge_attr = get_conn_attr(N7,E7)\n",
        "        list1=NE[10:21]\n",
        "        list2=[NE[7]]\n",
        "        loc=list1+list2\n",
        "        t=duration[index-81*7]\n",
        "        y = salinity[7,index-7*81+TIME_WINDOW:index-7*81+TIME_WINDOW+PRED_TIME]\n",
        "        val_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "\n",
        "    while ((8*81-PRED_TIME-TIME_WINDOW)<index<8*81):\n",
        "        index +=1 \n",
        "#O10        \n",
        "    while ( 8*81<=index<=(9*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[[10,11,12,13,14,15,16,17,18,19,20,8],index-8*81:index-8*81+TIME_WINDOW]\n",
        "        l=level[[10,11,12,13,14,15,16,17,18,19,20,8],index-8*81:index-8*81+TIME_WINDOW]\n",
        "        p=pump[[10,11,12,13,14,15,16,17,18,19,20,8],index-8*81:index-8*81+TIME_WINDOW]\n",
        "        i=infil[[10,11,12,13,14,15,16,17,18,19,20,8],index-8*81:index-8*81+TIME_WINDOW]\n",
        "        i_s=infil_s[[10,11,12,13,14,15,16,17,18,19,20,8],index-8*81:index-8*81+TIME_WINDOW]\n",
        "        r = rain[[10,11,12,13,14,15,16,17,18,19,20,8],index-8*81:index-8*81+TIME_WINDOW]\n",
        "        N8= North[10:21]+[North[8]]\n",
        "        E8= East[10:21]+[East[8]]\n",
        "        conn,edge_attr = get_conn_attr(N8,E8)\n",
        "        list1=NE[10:21]\n",
        "        list2=[NE[8]]\n",
        "        loc=list1+list2\n",
        "        t=duration[index-81*8]\n",
        "        y = salinity[8,index-8*81+TIME_WINDOW:index-8*81+TIME_WINDOW+PRED_TIME]\n",
        "        train_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "    while ((9*81-PRED_TIME-TIME_WINDOW)<index<9*81):\n",
        "        index +=1 \n",
        "        \n",
        "#O11        \n",
        "    while ( 9*81<=index<=(10*81-PRED_TIME-TIME_WINDOW)):\n",
        "        s=salinity[[10,11,12,13,14,15,16,17,18,19,20,9],index-9*81:index-9*81+TIME_WINDOW]\n",
        "        l=level[[10,11,12,13,14,15,16,17,18,19,20,9],index-9*81:index-9*81+TIME_WINDOW]\n",
        "        p=pump[[10,11,12,13,14,15,16,17,18,19,20,9],index-9*81:index-9*81+TIME_WINDOW]\n",
        "        i=infil[[10,11,12,13,14,15,16,17,18,19,20,9],index-9*81:index-9*81+TIME_WINDOW]\n",
        "        i_s=infil_s[[10,11,12,13,14,15,16,17,18,19,20,9],index-9*81:index-9*81+TIME_WINDOW]\n",
        "        r = rain[[10,11,12,13,14,15,16,17,18,19,20,9],index-9*81:index-9*81+TIME_WINDOW]\n",
        "        N9= North[10:21]+[North[9]]\n",
        "        E9= East[10:21]+[East[9]]\n",
        "        conn,edge_attr = get_conn_attr(N9,E9)\n",
        "        list1=NE[10:21]\n",
        "        list2=[NE[9]]\n",
        "        loc=list1+list2\n",
        "        t=duration[index-81*9]\n",
        "        y = salinity[9,index-9*81+TIME_WINDOW:index-9*81+TIME_WINDOW+PRED_TIME]\n",
        "        test_sequences.append((s,l,p,i,i_s,r,loc,t,conn,edge_attr,y))\n",
        "        index+=1\n",
        "    while ((10*81-PRED_TIME-TIME_WINDOW)<index<10*81):\n",
        "        index +=1 \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "    return train_sequences,val_sequences,test_sequences"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Uj5P4C0XpEx",
        "outputId": "5c618706-be32-4fbb-e64c-602d57f1d285"
      },
      "source": [
        "salinity[[10,11,12,13,14,15,16,17,18,19,20,3],250:250-3*81+TIME_WINDOW]"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([], shape=(12, 0), dtype=float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIKMVNachP_j",
        "outputId": "846dedf8-b4a8-4a63-a14b-fa08551b2d51"
      },
      "source": [
        "train_sequences,val_sequences, test_sequences = create_dataset(salinity,level,pump,infil,infil_s,rain,NE,duration)\n",
        "print(len(train_sequences))\n",
        "print(len(test_sequences))\n",
        "print(len(val_sequences))\n",
        "print(train_sequences[122][10])"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "276\n",
            "92\n",
            "92\n",
            "[1.56 1.56 1.54 1.51 1.53 1.53 1.52 1.5  1.48 1.47 1.44 1.42 1.4  1.38\n",
            " 1.35 1.33 1.31 1.27]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQrltlNhhw3p"
      },
      "source": [
        "## 1.3 Create datamodule and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_QpOuuKhl6z"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class WellDataset(Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        self.sequences = sequences\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    def __getitem__(self,index):\n",
        "        sal,lev,pum,inf,inf_s,rai,loca,dur,conn,edge_attr, pred_y = self.sequences[index]\n",
        "        return dict(\n",
        "            sal_sequence = torch.tensor(sal),\n",
        "            lev_sequence = torch.tensor(lev),\n",
        "            pum_sequence = torch.tensor(pum),\n",
        "            inf_sequence = torch.tensor(inf),\n",
        "            inf_s_sequence = torch.tensor(inf_s),\n",
        "            rai_sequence = torch.tensor(rai),\n",
        "            loc_sequence = torch.tensor(loca),\n",
        "            dur_sequence = torch.tensor(dur),\n",
        "            con_sequence = torch.tensor(conn),\n",
        "            edg_sequence = torch.tensor(edge_attr),\n",
        "            \n",
        "            label = torch.tensor(pred_y).float()   \n",
        "        )\n",
        "\n",
        "    \n",
        "\n",
        "class WellDataModule(pl.LightningDataModule):\n",
        "    \n",
        "    def __init__(\n",
        "        self, train_sequences, val_sequences, test_sequences, batch_size=45\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.train_sequences = train_sequences\n",
        "        self.val_sequences = val_sequences\n",
        "        self.test_sequences = test_sequences\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "    def setup(self):\n",
        "        self.train_dataset = WellDataset(self.train_sequences)\n",
        "        self.val_dataset = WellDataset(self.val_sequences)\n",
        "        self.test_dataset = WellDataset(self.test_sequences)\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle = False\n",
        "        )\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "        self.val_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle = False\n",
        "        )\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "        self.test_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle = False\n",
        "        )"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUsG9Fgwh8v1"
      },
      "source": [
        "N_EPOCHS=8\n",
        "BATCH_SIZE= 46\n",
        "##We set the batchsize within a single ob well data, which is 81-18-18+1=46. \n",
        "data_module = WellDataModule(train_sequences,val_sequences, test_sequences,batch_size =BATCH_SIZE )\n",
        "data_module.setup()"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPoQ7mQDiFS3",
        "outputId": "cc4f93b4-a4a8-471e-fc71-c5eaea8438a9"
      },
      "source": [
        "train_dataset =  WellDataset(train_sequences)\n",
        "for item in train_dataset:\n",
        "    print(item.keys())\n",
        "    print(item['sal_sequence'].shape)\n",
        "    break"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['sal_sequence', 'lev_sequence', 'pum_sequence', 'inf_sequence', 'inf_s_sequence', 'rai_sequence', 'loc_sequence', 'dur_sequence', 'con_sequence', 'edg_sequence', 'label'])\n",
            "torch.Size([10, 18])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdNSW_rE4FN8",
        "outputId": "67cf6554-3438-4e46-acdb-8c3faa87ceec"
      },
      "source": [
        "print(train_dataset[1]['con_sequence'])"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0,  1,  3,  4,  5,  6,  8,  8,  9,  9,  9,  9,  9, 10],\n",
            "        [ 9,  9,  8,  9,  9,  9,  3, 10,  0,  1,  4,  5,  6,  8]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QuugqK4iHtq"
      },
      "source": [
        "# 2.Model\n",
        "\n",
        "### I set sal_em=8,lev_em=8,pum_em=8,inf_em=8,inf_s_em=8,rai_em=8, loc_em=8 dur_em=8. Then node_h=64, edge_h=1. For each feature, the embed-form dimension should be 46 \\*12 \\*18 \\*8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FggPMKfrqOT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
        "from torch_scatter import scatter_mean, scatter_add\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_xKF1cWrvo0"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
        "from torch_scatter import scatter_mean, scatter_add\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class RNNEncoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(RNNEncoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size,\n",
        "                            hidden_size,\n",
        "                            num_layers,\n",
        "                            batch_first=True)\n",
        "\n",
        "    def forward(self, x, h0, c0):\n",
        "        # Set initial hidden and cell states\n",
        "        # Forward propagate LSTM\n",
        "        out, (h_n, c_n) = self.lstm(x, (h0, c0))  \n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        return h_n, c_n\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size,\n",
        "                            hidden_size,\n",
        "                            num_layers,\n",
        "                            batch_first=True)\n",
        "        self.lin = nn.Linear(hidden_size, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, h_0, c_0):\n",
        "        # Forward propagate LSTM\n",
        "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        out = self.lin(out)\n",
        "        out = self.relu(out)\n",
        "        # Decode the hidden state of the last time step\n",
        "        return out, h_n, c_n\n",
        "class WellModel(nn.Module):\n",
        "    def __init__(self, sal_em, lev_em, pum_em, inf_em, inf_s_em,rai_em, loc_em,dur_em, rnn_h, rnn_l, gnn_h):\n",
        "        super(WellModel, self).__init__()\n",
        "        self.sal_em = sal_em\n",
        "        self.lev_em=lev_em\n",
        "        self.pum_em=pum_em\n",
        "        self.inf_em=inf_em\n",
        "        self.inf_s_em=inf_s_em\n",
        "        self.rai_em=rai_em\n",
        "        self.loc_em=loc_em\n",
        "        self.dur_em=dur_em\n",
        "        self.node_h=64\n",
        "        self.edge_h=2\n",
        "        self.well_num=12\n",
        "      \n",
        "        self.rnn_h=rnn_h\n",
        "        self.gnn_h=gnn_h\n",
        "        self.rnn_l=rnn_l\n",
        "\n",
        "        self.embed = nn.Sequential(\n",
        "          nn.Linear(1,8),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "        print(\"embed finished\",self.embed)\n",
        "        \n",
        "        self.sal_embed = nn.Sequential(\n",
        "          nn.Linear(1,8),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "        self.lev_embed = nn.Sequential(\n",
        "          nn.Linear(1,8),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "        self.pum_embed =nn.Sequential(\n",
        "          nn.Linear(1,8),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "        self.inf_embed =nn.Sequential(\n",
        "          nn.Linear(1,8),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "        self.inf_s_embed =nn.Sequential(\n",
        "          nn.Linear(1,8),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "        self.rai_embed =nn.Sequential(\n",
        "          nn.Linear(1,8),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "        self.loc_embed =nn.Sequential(\n",
        "          nn.Linear(2,8),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "        self.dur_embed =nn.Sequential(\n",
        "          nn.Linear(1,8),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "        '''\n",
        "        self.sal_embed = Seq(Lin(1, 8), ReLU()) \n",
        "        self.lev_embed = Seq(Lin(1, 8), ReLU())\n",
        "        self.pum_embed = Seq(Lin(1, 8), ReLU())\n",
        "        self.inf_embed = Seq(Lin(1, 8), ReLU())\n",
        "        self.inf_s_embed = Seq(Lin(1, 8), ReLU())\n",
        "        self.rai_embed = Seq(Lin(1, 8), ReLU())\n",
        "        self.loc_embed = Seq(Lin(2, 8), ReLU())\n",
        "        self.dur_embed = Seq(Lin(1, 8), ReLU())'''\n",
        "                                        ## set sal_em=8,lev_em=8,pum_em=8,inf_em=8,inf_s_em=8,rai_em=8, loc_em=8,dur_em=8 then ,node_h=64\n",
        "        self.well_gnn = WellGNN(64, 1, gnn_h) # need to do the final: my edge \n",
        "                                                             #feature is 1, node feature is the sum of ems,\n",
        "        self.encoder = RNNEncoder(input_size=gnn_h,\n",
        "                                  hidden_size=rnn_h,\n",
        "                                  num_layers=rnn_l)\n",
        "        self.decoder_embed =nn.Sequential(\n",
        "          nn.Linear(1,8),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "        self.decoder = RNNDecoder(input_size=8 + sal_em,\n",
        "                                  hidden_size=rnn_h,\n",
        "                                  num_layers=rnn_l)\n",
        "    def batchInput(self, x, edge_w, edge_conn):\n",
        "        well = x.shape[1]\n",
        "        x = x.reshape(-1, x.shape[-1])\n",
        "        edge_w = edge_w.reshape(-1, edge_w.shape[-1])\n",
        "        for i in range(edge_conn.size(0)):\n",
        "            edge_conn[i, :] = torch.add(edge_conn[i, :], i * well_num)\n",
        "        edge_conn = edge_conn.transpose(0, 1)  #need to adjust \n",
        "        edge_conn = edge_conn.reshape(2, -1)\n",
        "        return x, edge_w, edge_conn\n",
        "    \n",
        "    def forward(self, sequences_data):\n",
        "        #sta_aqi, sta_conn, sta_poi, sta_w, sta_wea, sta_for, _ = city_data\n",
        "        well_sal,well_lev, well_pum, well_inf, well_inf_s, rain_data, well_loc, well_dur, well_conn, well_w, _ = sequences_data \n",
        "        well_num = 12\n",
        "        print('this is well sal',well_sal.shape)\n",
        "        well_a = well_sal.unsqueeze(dim=-1).to(torch.float32)\n",
        "        print('this is well_a unsqueeze',type(well_a))\n",
        "        well_a = self.embed(well_a)   # 46*12*18* em_dim\n",
        "        print('this is well sal.shape',well_a.shape)\n",
        "        \n",
        "        well_b = well_lev.unsqueeze(dim=-1).to(torch.float32)\n",
        "        well_b = self.lev_embed(well_b)      \n",
        "        print('well_b',well_b.shape)\n",
        "        well_c = well_pum.unsqueeze(dim=-1).to(torch.float32)\n",
        "        well_c = self.pum_embed(well_c)\n",
        "        print('well_c',well_c.shape)\n",
        "        well_d = well_inf.unsqueeze(dim=-1).to(torch.float32)\n",
        "        well_d = self.inf_embed(well_d)\n",
        "        print('well_d',well_d.shape)\n",
        "        well_e = well_inf_s.unsqueeze(dim=-1).to(torch.float32)\n",
        "        well_e = self.inf_s_embed(well_e)\n",
        "        print('well_e',well_e.shape)\n",
        "        well_f = rain_data.unsqueeze(dim=-1).to(torch.float32)\n",
        "        well_f = self.rai_embed(well_f)\n",
        "        print('well_f',well_f.shape)\n",
        "        print(\"this is well loc\",well_loc.shape)\n",
        "        well_g = well_loc.unsqueeze(dim=-2).to(torch.float32)\n",
        "        well_g = self.loc_embed(well_g)\n",
        "        well_g = well_g.unsqueeze(dim=-3).repeat(1,well_num,18,1)\n",
        "        print('well_g',well_g.shape)       \n",
        "        \n",
        "        well_h = well_dur.unsqueeze(dim=-1).to(torch.float32)\n",
        "        well_h = self.dur_embed(well_h) \n",
        "        well_h = well_h.unsqueeze(dim=-2)\n",
        "        well_h = well_h.unsqueeze(dim=-3).repeat(1,well_num,18,1)\n",
        "        print('well_h',well_h.shape)\n",
        "        x = torch.cat([well_a,well_b,well_c,well_d,well_e,well_f,well_g,well_h],dim=-1)\n",
        "        x = x.transpose(1, 2) # (batch, well_num, sequence length, emb) to (batch, sequence, well_num, emb)\n",
        "        x = x.reshape(-1,x.shape[-2],x.shape[-1]) # to (nodes, well_num, emb)\n",
        "        well_conn = well_conn.repeat(18, 1, 1).to(torch.float32) # I did not transpose\n",
        "        well_w = well_w.reshape(-1, well_w.shape[-2], well_w.shape[-1]).to(torch.float32) # the first dim batch*\n",
        "        print('This is x.shape,well_conn.shape,well_w.shape',x.shape,well_conn.shape,well_w.shape)\n",
        "        x, well_weight, well_conn = self.batchInput(x, well_w, well_conn)\n",
        "        x = self.well_gnn(x, well_conn, well_weight, well_num)\n",
        "        x = x.reshape(-1, 18, well_num, x.shape[-1]).transpose(1, 2)\n",
        "        x = x.reshape(-1, 18, x.shape[-1])\n",
        "        \n",
        "        h0 = torch.randn(self.rnn_l, sta_x.size(0), self.rnn_h)\n",
        "        c0 = torch.randn(self.rnn_l, sta_x.size(0), self.rnn_h)\n",
        "        h_x, c_x = self.encoder(x, h0, c0)\n",
        "\n",
        "        outputs = torch.zeros((x.size(0), 18, 1)).to(device)\n",
        "        sal = well_sal[:,:,-1].reshape(-1,1)\n",
        "        well_loc_for = well_loc.repeat(well_num, 1,1)\n",
        "\n",
        "        for i in range(18):\n",
        "            sal_em = self.decoder_embed(sal)\n",
        "            inputs = torch.cat((sal_em, well_loc_for[:, i]), dim=-1) # well_loc em = 4\n",
        "            inputs = inputs.unsqueeze(dim=1)\n",
        "            output, h_x, c_x = self.decoder(inputs, h_x, c_x)\n",
        "            output = output.reshape(-1, 1)\n",
        "            outputs[:, i] = output\n",
        "            sal = output            \n",
        "        outputs = outputs.reshape(-1, well_num, well_for.size(1))\n",
        "\n",
        "        return outputs\n",
        "class WellGNN(torch.nn.Module):\n",
        "    def __init__(self, node_h, edge_h, gnn_h):\n",
        "        super(WellGNN, self).__init__()\n",
        "        self.node_mlp_1 =nn.Sequential(\n",
        "          nn.Linear(2 * node_h + edge_h, gnn_h),\n",
        "          nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.node_mlp_2 =nn.Sequential(\n",
        "          nn.Linear(node_h + gnn_h, gnn_h),\n",
        "          nn.ReLU(inplace=True)\n",
        "        )\n",
        "      \n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr,sta_num):\n",
        "        # x: [N, F_x], where N is the number of nodes.\n",
        "        # edge_index: [2, E] with max entry N - 1.\n",
        "        # edge_attr: [E, F_e]\n",
        "        #u = u.reshape(-1, u.shape[-1])\n",
        "        #u = u.repeat(sta_num, 1)\n",
        "        row, col = edge_index\n",
        "        row=row.to(torch.float32)\n",
        "        col = col.to(torch.float32)\n",
        "\n",
        "        out = torch.cat([x[row], x[col], edge_attr], dim=1)\n",
        "        out = self.node_mlp_1(out)\n",
        "        out = scatter_mean(out, col, dim=0, dim_size=x.size(0))\n",
        "        out = torch.cat([x, out], dim=1) #delete u\n",
        "        return self.node_mlp_2(out)"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZS08Yzq-wNS",
        "outputId": "be62259c-db7e-4135-ae82-98cb1d5fe3e1"
      },
      "source": [
        "from torch import nn\n",
        "sal_embed = nn.Sequential(\n",
        "          nn.Linear(1,8),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "\n",
        "a = torch.rand(46,12,18,1)\n",
        "a = sal_embed(a)\n",
        "print(a.shape)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([46, 12, 18, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFcK0ZtT2Xeh",
        "outputId": "e5e6cb42-da18-4fdc-9de2-f33e4d865517"
      },
      "source": [
        "sal_em=8,\n",
        "lev_em=8,\n",
        "pum_em=8,\n",
        "inf_em=8,\n",
        "inf_s_em=8,\n",
        "rai_em=8,\n",
        "loc_em=8,\n",
        "dur_em=8,\n",
        "node_h=64,\n",
        "edge_h=1,\n",
        "well_num=12,\n",
        "batch_size = 46,\n",
        "rnn_h=64,\n",
        "gnn_h=32,\n",
        "rnn_l=1,\n",
        "lr=0.001,\n",
        "epoch=10\n",
        "wells_model = WellModel(8, 8, 8, 8, 8,8, 8,8, 64, 1, 32)\n",
        "model_num = sum(p.numel() for p in wells_model.parameters()\n",
        "                      if p.requires_grad)\n",
        "print('model:', 'Trainable,', model_num)\n",
        "criterion = nn.MSELoss()\n",
        "params = wells_model.parameters()\n",
        "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=0.001)\n",
        "\n",
        "val_loss_min = np.inf"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embed finished Sequential(\n",
            "  (0): Linear(in_features=1, out_features=8, bias=True)\n",
            "  (1): ReLU()\n",
            ")\n",
            "model: Trainable, 53577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "-f4fV1pw6AZE",
        "outputId": "e0a11c4c-580f-4b32-de58-1562fa3c72b1"
      },
      "source": [
        "well_num=12\n",
        "Epoch=1\n",
        "batch_size=46\n",
        "data_module = WellDataModule(train_sequences,val_sequences, test_sequences,batch_size)\n",
        "data_module.setup()\n",
        "for epoch in range(Epoch):\n",
        "    #for i ,(well_sal,well_lev, well_pum, well_inf, well_inf_s, rain_data, \n",
        "    #        well_loc, well_dur, well_conn, well_w,pred_y) in enumerate(data_module.train_dataloader()):\n",
        "    for i,j in enumerate(data_module.train_dataloader()):\n",
        "      print(j.keys())\n",
        "      well_sal = j['sal_sequence']   \n",
        "      well_lev = j['lev_sequence']\n",
        "      well_pum = j['pum_sequence']\n",
        "      well_inf = j['inf_sequence'] \n",
        "      well_inf_s = j['inf_s_sequence'] \n",
        "      rain_data = j['rai_sequence']\n",
        "      well_loc = j['loc_sequence']  \n",
        "      well_dur = j['dur_sequence']\n",
        "      well_conn = j['con_sequence']\n",
        "      well_w = j['edg_sequence']\n",
        "      pred_y = j['label']\n",
        "\n",
        "      sequences = [well_sal,well_lev, well_pum, well_inf, well_inf_s, rain_data, \n",
        "                well_loc, well_dur, well_conn, well_w,pred_y]\n",
        "      outputs = wells_model(sequences)\n",
        "      loss = criterion(outputs, pred_y ) \n",
        "      break\n",
        "      \n",
        "\n",
        "\n",
        "      wells_model.zero_grad()\n",
        "\n",
        "      break"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['sal_sequence', 'lev_sequence', 'pum_sequence', 'inf_sequence', 'inf_s_sequence', 'rai_sequence', 'loc_sequence', 'dur_sequence', 'con_sequence', 'edg_sequence', 'label'])\n",
            "this is well sal torch.Size([46, 12, 18])\n",
            "this is well_a unsqueeze <class 'torch.Tensor'>\n",
            "this is well sal.shape torch.Size([46, 12, 18, 8])\n",
            "well_b torch.Size([46, 12, 18, 8])\n",
            "well_c torch.Size([46, 12, 18, 8])\n",
            "well_d torch.Size([46, 12, 18, 8])\n",
            "well_e torch.Size([46, 12, 18, 8])\n",
            "well_f torch.Size([46, 12, 18, 8])\n",
            "this is well loc torch.Size([46, 2])\n",
            "well_g torch.Size([46, 12, 18, 8])\n",
            "well_h torch.Size([46, 12, 18, 8])\n",
            "This is x.shape,well_conn.shape,well_w.shape torch.Size([828, 12, 64]) torch.Size([828, 2, 16]) torch.Size([828, 16, 1])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-161-13378368947a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m       sequences = [well_sal,well_lev, well_pum, well_inf, well_inf_s, rain_data, \n\u001b[1;32m     24\u001b[0m                 well_loc, well_dur, well_conn, well_w,pred_y]\n\u001b[0;32m---> 25\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwells_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_y\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-159-eecbf6390c9e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sequences_data)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This is x.shape,well_conn.shape,well_w.shape'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwell_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwell_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwell_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwell_conn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwell_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwell_conn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwell_gnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwell_conn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwell_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwell_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwell_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-159-eecbf6390c9e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, sta_num)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_mlp_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLoNVGvexe7R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}